{"cells":[{"cell_type":"code","source":["import re\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import ArrayType, StringType"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["MY_AWS_ACCESS_KEY = \"YOUR_AWS_ACCESS_KEY_HERE\"\nMY_AWS_SECRET_KEY = \"YOUR_AWS_SECRET_HERE\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import urllib\nACCESS_KEY = MY_AWS_ACCESS_KEY\nSECRET_KEY = MY_AWS_SECRET_KEY\nENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\nAWS_BUCKET_NAME = \"bbuzz2016\" # AWS_BUCKET_NAME = \"bbuzz2016data\" \nMOUNT_NAME = \"bbuzz2016\" # MOUNT_NAME = \"bbuzz2016data\"\ndbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/bbuzz2016\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["bbuzz = sqlContext.read.json(\"dbfs:/mnt/bbuzz2016/all-years.jsonlines\")\nbbuzz.registerTempTable(\"bbuzz_raw\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT * FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(sql(\"SELECT lower(content) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def cleanString(x):\n    return re.sub('\\s+', ' ', re.sub('[^a-zA-Z0-9 ]', '', x)).strip()\nsqlContext.registerFunction('cleanString', cleanString)\n\ndisplay(sql(\"SELECT cleanString(lower(content)) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["stopwords = set(sc.textFile(\"dbfs:/mnt/bbuzz2016/stopwords.txt\").collect())\ndef removeStopwords(x):\n    return ' '.join([word for word in x.split(' ') if word not in stopwords])\nsqlContext.registerFunction('removeStopwords', removeStopwords)\n#stopwords\n#display(sqlContext.sql(\"SELECT removeStopwords(cleanString(lower(content))) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))\n# -> See https://issues.apache.org/jira/browse/SPARK-11159\n\nsqlContext.registerFunction('clearAll', lambda x: removeStopwords(cleanString(x)))\ndisplay(sql(\"SELECT clearAll(lower(content)) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Cooler: http://www.nltk.org/index.html\n#wnl = WordNetLemmatizer()\n#wnl.lemmatize(x)\n# Alternatives:\n# * http://nlp.stanford.edu/software/\n# * https://opennlp.apache.org/ \ndef lemmatize(x):\n    return ' '.join([re.sub('s$', '', word) for word in x.split(' ')])\nsqlContext.registerFunction('lemmatize', lemmatize)\n\nsqlContext.registerFunction('clearAll', lambda x: lemmatize(removeStopwords(cleanString(x))))\ndisplay(sql(\"SELECT clearAll(lower(content)) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["pairs = [\n  'big data',\n  'open source',\n  'machine learning',\n  'real time',\n  'berlin buzzword',\n  'search engine',\n  'data store',\n  'event sourcing',\n  'map reduce'\n]\ndef tokenize(x):\n    for pair in pairs:\n        x = x.replace(pair, pair.replace(' ', ''))\n    return x\nsqlContext.registerFunction('tokenize', tokenize)\n\nsqlContext.registerFunction('clearAll', lambda x: tokenize(lemmatize(removeStopwords(cleanString(x)))))\ndisplay(sql(\"SELECT clearAll(lower(content)) FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(sql(\"SELECT link FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(sql(\"SELECT regexp_extract(regexp_replace(link, 'www\\.berlinbuzzwords\\.de', '2016.berlinbuzzwords.de'), '([0-9]+)\\.berlinbuzzwords', 1), link FROM bbuzz_raw WHERE content <> '' LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["bbuzz_websites = sqlContext.sql(\"\"\"\nSELECT\n  link,\n  speakers,\n  regexp_extract(regexp_replace(link, 'www\\.berlinbuzzwords\\.de', '2016.berlinbuzzwords.de'), '([0-9]+)\\.berlinbuzzwords', 1) as year,\n  cleanString(lower(title)) as title,\n  cleanString(lower(content)) as content,\n  clearAll(lower(concat(title, ' ', content))) as body\nFROM bbuzz_raw\n\"\"\")\nbbuzz_websites.registerTempTable(\"bbuzz_websites\")\ndisplay(sql(\"SELECT * FROM bbuzz_websites LIMIT 10\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["sessions = sqlContext.sql(\"\"\"\n  SELECT * FROM bbuzz_websites\n  WHERE\n    (link LIKE '%.de/content%' OR link LIKE '%.de/session%')\n    AND NOT (title LIKE 'lunch%' OR title LIKE 'coffee break%' OR title LIKE 'lightning talk%' OR title LIKE '% hackathon' OR title LIKE '% workshop' OR title LIKE '% barcamp')\n\"\"\")\nsessions.registerTempTable(\"sessions\")\ndisplay(sql(\"SELECT link, title FROM sessions WHERE year = 2015\"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(sessions.groupBy(\"year\").count().orderBy(\"year\"))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(sqlContext.createDataFrame(\n    sessions.select(\"speakers\")\n            .flatMap(lambda line: line.speakers)\n            .map(lambda x: (x, 1))\n            .reduceByKey(lambda a, b: a + b)\n            .map(lambda x: Row(speaker=x[0], talks=x[1]))\n).orderBy('talks', ascending=False))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["counts = sqlContext.createDataFrame(\n    sessions.select(\"body\") \\\n            .flatMap(lambda line: line.body.split(\" \")) \\\n            .map(lambda word: (word, 1)) \\\n            .reduceByKey(lambda a, b: a + b) \\\n            .map(lambda a: Row(word=a[0], count=a[1])) \\\n)\ncounts.registerTempTable(\"wordcounts\")\ntop_counts = sqlContext.sql(\"SELECT word, count FROM wordcounts ORDER BY count DESC LIMIT 50\")\ndisplay(top_counts)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["counts = sqlContext.createDataFrame(\n    sessions.select('year', 'body') \\\n            .flatMap(lambda line: [(line.year, word) for word in line.body.split(\" \")]) \\\n            .map(lambda line: ((line[0], line[1]), 1)) \\\n            .reduceByKey(lambda a, b: a + b) \\\n            .map(lambda a: Row(year=a[0][0], word=a[0][1], count=a[1])) \\\n)\ncounts.registerTempTable(\"wordcounts_per_year\")\ntop_counts = sqlContext.sql(\"SELECT year, word, count FROM wordcounts_per_year ORDER BY count DESC LIMIT 50\")\ndisplay(top_counts)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["def timeline(terms):\n    return sqlContext.sql(\"\"\"\nSELECT year, word, count FROM wordcounts_per_year\nWHERE word IN ('%s')\nORDER BY year, count ASC\n\"\"\" % (\"', '\".join(terms)))\n\ndef timelineRelative(terms):\n    return sqlContext.sql(\"\"\"\nSELECT w.year, word, count, count / total * 100 as relative_count FROM wordcounts_per_year w LEFT JOIN (SELECT year, sum(count) as total FROM wordcounts_per_year GROUP BY year) t ON (w.year = t.year)\nWHERE word IN ('%s')\nORDER BY year, count ASC\n\"\"\" % (\"', '\".join(terms))) "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(timeline([\n  'lucene',\n  'solr',\n  'elasticsearch',\n]))   "],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(timelineRelative([\n  'lucene',\n  'solr',\n  'elasticsearch',\n]))   "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(timelineRelative([\n  'storm',\n  'flink',\n  'spark',\n]))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(timelineRelative([\n  'hadoop',\n  'mapreduce',\n  'hdf',\n  'yarn',\n  'nosql',\n  'sql',\n]))  "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(timelineRelative([\n  'nosql',\n  'sql',\n  'graph',\n]))   "],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(timelineRelative([\n  'cassandra',\n  'hbase',\n  'redi',\n  'riak',\n  'couchdb',\n  'mongodb',\n]))   "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(timelineRelative([\n  'streaming',\n  'realtime',\n  'batch',\n]))   "],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(timelineRelative([\n  'hive',\n  'pig',\n  'impala',\n  'presto',\n  'sparksql',\n  'drill',\n]))   "],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(timelineRelative([\n  'crunch',\n  'eventsourcing',\n  'blockchain',\n]))   "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["old = ['hadoop', 'mapreduce', 'hdf']\nnew = ['spark', 'flink', 'storm']\n\ndisplay(sqlContext.sql(\"\"\"\nSELECT w.year, CASE WHEN word in ('%s') THEN 'hadoop+mapreduce+hdfs' ELSE 'spark+flink+storm' END AS technology, count, count / total * 100 as relative_count\nFROM wordcounts_per_year w LEFT JOIN (SELECT year, sum(count) as total FROM wordcounts_per_year GROUP BY year) t ON (w.year = t.year)\nWHERE word IN ('%s')\nORDER BY year, count ASC\n\"\"\" % (\"', '\".join(old), \"', '\".join(old + new))))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["def stripArray(x):\n    return [element.lower().strip() for element in x]\nsqlContext.registerFunction(\"stripArray\", stripArray, ArrayType(StringType()))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT title, body FROM bbuzz_websites WHERE year = 2016 and (link LIKE '%/content%' OR link LIKE '%/session%') LIMIT 50\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT year, count(distinct link) as urls, sum(length(body)) as text_length, sum(length(body)) / count(distinct link) avg_document_size FROM bbuzz_websites GROUP BY year ORDER BY year\"))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["display(dbutils.fs.ls('/databricks-datasets/cs100/lab3/data-001/'))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT year, link, body FROM bbuzz_websites WHERE year = 2010 LIMIT 100\"))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT year, count(distinct link) as urls, sum(length(body)) as text_length, sum(length(body)) / count(distinct link) avg_document_size FROM bbuzz_websites WHERE (link LIKE '%/content%' OR link LIKE '%/session%') GROUP BY year ORDER BY year\"))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(sqlContext.sql(\"SELECT year, link, speakers, title FROM sessions WHERE year = 2012 ORDER BY title\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["sqlContext.registerFunction(\"array_size\", lambda x: len(x))\ndisplay(sqlContext.sql(\"SELECT year, sum(array_size(split(body, ' '))) as number_of_words FROM sessions GROUP BY year ORDER BY year\"))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["wordcounts = sqlContext.createDataFrame(sessions.select(\"body\") \\\n        .flatMap(lambda line: line.body.split(\" \")) \\\n        .map(lambda word: (word, 1)) \\\n        .reduceByKey(lambda a, b: a + b) \\\n        .map(lambda a: Row(word=a[0], count=a[1]))) \\\n        .sort(\"count\", ascending=False)\n\n\n#wordcounts.select(\"word\").rdd.map(lambda x: x[0]).collect()\n\n#http://stackoverflow.com/questions/34423281/spark-dataframe-word-count-per-document-single-row-per-document\n\n#display(sqlContext.createDataFrame(sessions.select(\"year\").distinct().map(wordcount)))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["counts = sqlContext.createDataFrame(\n    sessions.select(\"body\") \\\n            .flatMap(lambda line: line.body.split(\" \")) \\\n            .map(lambda word: (word, 1)) \\\n            .reduceByKey(lambda a, b: a + b) \\\n            .map(lambda a: Row(word=a[0], count=a[1])) \\\n)\ncounts.registerTempTable(\"wordcounts\")\ntop_counts = sqlContext.sql(\"SELECT word, count FROM wordcounts ORDER BY count DESC LIMIT 50\")\ndisplay(top_counts)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["top_counts = sqlContext.sql(\"SELECT word, count FROM wordcounts ORDER BY count DESC LIMIT 50\")\ndisplay(top_counts)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["counts = sqlContext.createDataFrame(\n    sessions.select(\"speakers\") \\\n            .flatMap(lambda line: line.speakers) \\\n            .map(lambda word: (word, 1)) \\\n            .reduceByKey(lambda a, b: a + b) \\\n            .map(lambda a: Row(word=a[0], count=a[1])) \\\n)\ncounts.registerTempTable(\"top_speakers\")\ntop_speakers = sqlContext.sql(\"SELECT word, count FROM top_speakers ORDER BY count DESC LIMIT 50\")\ndisplay(top_speakers)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.mllib.feature import HashingTF as MllibHashingTF, IDF as MllibIDF\nfrom pyspark.mllib.linalg import SparseVector"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["def run_tf_idf_spark_mllib(df, numFeatures=1 << 20):\n    tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n    wordsData = tokenizer.transform(df)\n\n    words = wordsData.select(\"words\").rdd.map(lambda x: x.words)\n\n    hashingTF = MllibHashingTF(numFeatures)\n    tf = hashingTF.transform(words)\n    tf.cache()\n\n    idf = MllibIDF().fit(tf)\n    tfidf = idf.transform(tf)\n\n    # @TODO make this nicer\n    tmp = sqlContext.createDataFrame(wordsData.rdd.zip(tfidf), [\"data\", \"features\"])\n    tmp.registerTempTable(\"tmp\")\n    old_columns = ', '.join(map(lambda x: 'data.%s' % x, wordsData.columns))\n    with_features = sqlContext.sql(\"SELECT %s, features FROM tmp\" % old_columns)\n    tmp = sqlContext.createDataFrame(with_features.rdd.zip(tf), [\"data\", \"rawFeatures\"])\n    tmp.registerTempTable(\"tmp\")\n    old_columns = ', '.join(map(lambda x: 'data.%s' % x, with_features.columns))\n    return sqlContext.sql(\"SELECT %s, rawFeatures FROM tmp\" % old_columns)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["def run_tf_idf_spark_ml(df, numFeatures=1 << 20):\n    tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n    wordsData = tokenizer.transform(df)\n\n    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=numFeatures)\n    featurizedData = hashingTF.transform(wordsData)\n\n    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n    idfModel = idf.fit(featurizedData)\n\n    return idfModel.transform(featurizedData)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["def idf_scores_for_terms(tfidf, group_key):\n\n    group_keys = tfidf.select(group_key).collect()\n    words = tfidf.select(\"words\").collect()\n    features = tfidf.select(\"features\").collect()\n    rawFeatures = tfidf.select(\"rawFeatures\").collect()\n    \n    numFeatures = features[0].features.size\n    mllibHashingTf = MllibHashingTF(numFeatures=numFeatures)\n    \n    idfscores = list()\n    for line_idx, feature_row in enumerate(features):\n        terms = dict()\n\n        for term_array in words[line_idx]:\n            for term in term_array:\n                term_index = mllibHashingTf.indexOf(term)\n                terms[term] = (feature_row.features[term_index], rawFeatures[line_idx].rawFeatures[term_index])\n        for term, score in terms.iteritems():\n            idfscores.append((group_keys[line_idx][group_key], term, float(score[0]), float(score[1])))\n\n    return idfscores"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["tfidf = run_tf_idf_spark_mllib(sessions)\nscores = sqlContext.createDataFrame(sc.parallelize(idf_scores_for_terms(tfidf, \"link\")), [\"link\", \"term\", \"score\", \"tf\"])\nscores.registerTempTable(\"scores\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["top_terms = sqlContext.sql(\"SELECT term, score, tf, link FROM scores WHERE link = 'https://www.berlinbuzzwords.de/session/live-hack-analyzing-7-years-buzzwords-scale' ORDER BY score DESC\")\ndisplay(top_terms)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["top_terms = sqlContext.sql(\"SELECT term, score, tf, link FROM scores WHERE link = 'http://2015.berlinbuzzwords.de/session/machine-learning-startup-big-data-company' ORDER BY score DESC\")\ndisplay(top_terms)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["def concat(a, b):\n  return '%s %s' % (str(a), str(b))\nsessions_per_year = sqlContext.createDataFrame(sessions.select(\"year\", \"body\").rdd.combineByKey(str, concat, concat).collect(), ['year', 'body'])\nsessions_per_year.registerTempTable(\"sessions_per_year\")\ndisplay(sessions_per_year)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["tfidf_per_year = run_tf_idf_spark_mllib(sessions_per_year)\n#display(tfidf_per_year.select(\"features\"))\nscores_per_year = sqlContext.createDataFrame(sc.parallelize(idf_scores_for_terms(tfidf_per_year, \"year\")), [\"year\", \"term\", \"score\", \"tf\"])\nscores_per_year.registerTempTable(\"scores_per_year\")"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["top_terms_per_year = sqlContext.sql(\"\"\"\nSELECT year, term, score, rank FROM (\n  SELECT year, term, score, dense_rank() OVER (PARTITION BY year ORDER BY score DESC) as rank\n  FROM scores_per_year\n  WHERE score <> 0\n) tmp\nWHERE\n  rank <= 10\nORDER BY year, rank ASC\n\"\"\")\ndisplay(top_terms_per_year)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# Prove that hashing functions are different in mllib and ml\nmllibHashingTf = MllibHashingTF(numFeatures=numFeatures)\n\ndef indexOf(term):\n    \"\"\" Returns the index of the input term. \"\"\"\n    return hash(term) % numFeatures\n\nwords = first[0].words\nfeatures = first[0].rawFeatures\nfor index, term in enumerate(words):\n    term_index = indexOf(term)\n    print '%s %s %d %f %s' % (index, term, term_index, features[term_index], term_index in features.indices)\n\nprint words\nprint features"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["def timeline(terms):\n    return sqlContext.sql(\"\"\"\nSELECT year, term, tf FROM scores_per_year\nWHERE term IN ('%s')\nORDER BY year, tf ASC\n\"\"\" % (\"', '\".join(terms))) "],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["def timelineRelative(terms):\n    return sqlContext.sql(\"\"\"\nSELECT s.year, term, tf, total, tf / total as relative_tf FROM scores_per_year s LEFT JOIN (SELECT year, sum(tf) as total FROM scores_per_year GROUP BY year) t ON (s.year = t.year)\nWHERE term IN ('%s')\nORDER BY year, tf ASC\n\"\"\" % (\"', '\".join(terms))) "],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["display(timelineRelative([\n  'lucene',\n  'solr',\n  'elasticsearch',\n]))   "],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["display(timeline([\n  'lucene',\n  'solr',\n  'elasticsearch',\n]))   "],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["display(timeline([\n  'storm',\n  'flink',\n  'spark',\n]))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["display(timeline([\n  'mapreduce',\n  'hdf',\n  'yarn',\n  'nosql',\n  'sql',\n]))   "],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["display(timeline([\n  'nosql',\n  'sql',\n  'graph',\n]))   "],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["display(timeline([\n  'cassandra',\n  'hbase',\n  'redi',\n  'riak',\n  'couchdb',\n  'mongodb',\n]))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["display(timeline([\n  'streaming',\n  'realtime',\n  'batch',\n]))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["display(timeline([\n  'hive',\n  'pig',\n  'impala',\n  'presto',\n  'sparksql',\n  'drill',\n]))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["display(timeline([\n  'crunch',\n  'eventsourcing',\n]))"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["old = ['hadoop', 'mapreduce', 'hdf']\nnew = ['spark', 'flink', 'storm']\n\ndisplay(sqlContext.sql(\"\"\"\nSELECT year, CASE WHEN term in ('%s') THEN 'hadoop+mapreduce+hdfs' ELSE 'spark+flink+storm' END AS technology, tf FROM scores_per_year\nWHERE term IN ('%s')\nORDER BY year, tf ASC\n\"\"\" % (\"', '\".join(old), \"', '\".join(old + new))))"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["display(sqlContext.sql(\"\"\"\nSELECT year, sum(tf) FROM scores_per_year\nGROUP BY year\nORDER BY year ASC\n\"\"\"))"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["display(sqlContext.sql(\"\"\"\nSELECT term, tf FROM scores_per_year\nWHERE year = 2016\nORDER BY tf DESC LIMIT 20\n\"\"\"))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["dbutils.fs.cp('/databricks-datasets/cs100/lab3/data-001/stopwords.txt', '/mnt/bbuzz2016/stopwords.txt')"],"metadata":{},"outputs":[],"execution_count":70}],"metadata":{"name":"bbuzz2016-backup","notebookId":183210672370542},"nbformat":4,"nbformat_minor":0}
